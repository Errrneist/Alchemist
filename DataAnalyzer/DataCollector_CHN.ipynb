{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 新闻数据采集器(国内板块)\n",
    "## 吴泓骏 [拂晓工作室](https://github.com/Errrneist/Alchemist)\n",
    "* 此程序将对指定股票数据进行收集并整理\n",
    "* 同时创建SFrame友好的CSV文件以及对数据进行清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "* [1] [Basics of SFrame](https://apple.github.io/turicreate/docs/api/generated/turicreate.SFrame.html#turicreate.SFrame)\n",
    "* [2] [Remove Multiple Substring from String](https://stackoverflow.com/questions/31273642/better-way-to-remove-multiple-words-from-a-string)\n",
    "* [3] [楼老师的Python分析红楼梦](https://zhuanlan.zhihu.com/p/29209681)\n",
    "* [4] [如何使用pyltp包进行中文分词](https://blog.csdn.net/sinat_26917383/article/details/77067515)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongjunwu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 导入库\n",
    "import urllib\n",
    "import re\n",
    "import pymysql\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import turicreate as tc\n",
    "import turicreate as tc\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备阶段\n",
    "* 1. 定义用于搜寻的板块\n",
    "* 2. 创建一个包含25个链接的搜寻目录\n",
    "* 3. 创建一个包含20个Class的搜寻目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功创建包含 25 个页面链接的目录！\n",
      "成功创建包含 20 个Class的目录！\n"
     ]
    }
   ],
   "source": [
    "# 新闻板块(同时也会加入热门新闻的数据)\n",
    "china = 'cgnjj'  # 国内\n",
    "international = 'cgjjj'  # 国际\n",
    "\n",
    "# 定义基本常量\n",
    "url = 'http://finance.eastmoney.com/news/' + china + '.html' # 主目录板块\n",
    "\n",
    "# 创建一个二十五页的list\n",
    "page_list = []\n",
    "counter = 1\n",
    "\n",
    "while counter <= 25:\n",
    "    pageurl = 'http://finance.eastmoney.com/news/' + china\n",
    "    if counter != 1:\n",
    "        pageurl = pageurl + '_' + str(counter) + '.html'\n",
    "        page_list.append(pageurl)\n",
    "    else:\n",
    "        pageurl = pageurl + '.html'\n",
    "        page_list.append(pageurl)\n",
    "    counter += 1\n",
    "    \n",
    "print('成功创建包含 ' + str(len(page_list)) + ' 个页面链接的目录！')\n",
    "\n",
    "# 创建一个包含20个class的list\n",
    "counter = 0\n",
    "class_list = []\n",
    "\n",
    "while counter < 20:\n",
    "    class_list.append('newsTr' + str(counter))\n",
    "    counter += 1\n",
    "    \n",
    "print('成功创建包含 ' + str(len(class_list)) + ' 个Class的目录！')\n",
    "# print(class_list)  # Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得所有文章的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------获取所有文章超链接程序-----------------\n",
      "分析任务开始！\n",
      "第1页分析完成！现在一共获得了38篇文章的链接！\n",
      "第2页分析完成！现在一共获得了76篇文章的链接！\n",
      "第3页分析完成！现在一共获得了114篇文章的链接！\n",
      "第4页分析完成！现在一共获得了154篇文章的链接！\n",
      "第5页分析完成！现在一共获得了193篇文章的链接！\n",
      "第6页分析完成！现在一共获得了232篇文章的链接！\n",
      "第7页分析完成！现在一共获得了272篇文章的链接！\n",
      "第8页分析完成！现在一共获得了313篇文章的链接！\n",
      "第9页分析完成！现在一共获得了352篇文章的链接！\n",
      "第10页分析完成！现在一共获得了393篇文章的链接！\n",
      "第11页分析完成！现在一共获得了433篇文章的链接！\n",
      "第12页分析完成！现在一共获得了473篇文章的链接！\n",
      "第13页分析完成！现在一共获得了515篇文章的链接！\n",
      "第14页分析完成！现在一共获得了557篇文章的链接！\n",
      "第15页分析完成！现在一共获得了597篇文章的链接！\n",
      "第16页分析完成！现在一共获得了636篇文章的链接！\n",
      "第17页分析完成！现在一共获得了675篇文章的链接！\n",
      "第18页分析完成！现在一共获得了713篇文章的链接！\n",
      "第19页分析完成！现在一共获得了751篇文章的链接！\n",
      "第20页分析完成！现在一共获得了791篇文章的链接！\n",
      "第21页分析完成！现在一共获得了834篇文章的链接！\n",
      "第22页分析完成！现在一共获得了875篇文章的链接！\n",
      "第23页分析完成！现在一共获得了914篇文章的链接！\n",
      "第24页分析完成！现在一共获得了952篇文章的链接！\n",
      "第25页分析完成！现在一共获得了992篇文章的链接！\n",
      "全部分析完成！正在查重...\n",
      "任务完成！共获得了516篇文章的链接！\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 初始化urllist\n",
    "urllist = []\n",
    "year = '2018'  # Separate parameter\n",
    "counter = 1\n",
    "\n",
    "# 创造urllist\n",
    "print('-----------------获取所有文章超链接程序-----------------')\n",
    "print('分析任务开始！')\n",
    "for url in page_list:\n",
    "    req = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for each_url in soup.find_all('a', href=True):\n",
    "        if 'http://finance.eastmoney.com/news/' in each_url['href']:\n",
    "            if year in each_url['href']:\n",
    "                urllist.append(each_url['href'])   \n",
    "    print('第' + str(counter) + '页分析完成！现在一共获得了' + str(len(urllist)) + '篇文章的链接！')\n",
    "    counter += 1\n",
    "\n",
    "print('全部分析完成！正在查重...')\n",
    "urllist = list(set(urllist))\n",
    "print('任务完成！共获得了' + str(len(urllist)) + '篇文章的链接！')\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将各个网页的资料抓取进SFrame\n",
    "* 1. 新建用于抓取的函数\n",
    "* 2. 实施抓取\n",
    "* 3. 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据的词库\n",
    "banned_info = ['责任编辑','原标题']\n",
    "banned_words = ['摘要\\n', '\\n', '\\r','\\u3000','（中国新闻网）',\n",
    "                '来源','以下简称', '（新华社）', '>>>', \n",
    "                '商务微新闻带你一图了解！','-', '经济日报', '中国经济网',\n",
    "                '附件：', '>>>>', '>>', '>>>>>', '（新华网）', \n",
    "                '（第一财经）', '据新华社报道，', '▼', '▲']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓取新闻URL函数\n",
    "def collectNews(news, url, counter):\n",
    "    # 准备请求数据\n",
    "    req = urllib.request.Request(url)\n",
    "    # print('请求链接连接成功!')\n",
    "    response = urllib.request.urlopen(req)\n",
    "    # print('收到反馈信号！')\n",
    "    html = response.read()\n",
    "    # print('HTML生成完毕！')\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # print('SOUP创建完成！')\n",
    "    \n",
    "    # 获得文章的发表时间 \n",
    "    time = soup.find(class_=\"time\").get_text()\n",
    "    # print('时间获取完毕！')\n",
    "    \n",
    "    # 获得文章的标题\n",
    "    title = soup.find('h1').get_text()\n",
    "    \n",
    "    # 鉴于有的文章将标题重复一遍 加入洗词库把标题洗掉\n",
    "    banned_words.append('【' + title + '】')\n",
    "    \n",
    "    # 获得文章的内容\n",
    "    content = soup.find(id=\"ContentBody\").get_text() \n",
    "    \n",
    "    # 清洗文章末尾编辑原文信息    \n",
    "    for banned_information in banned_info:\n",
    "        if banned_information in content:\n",
    "            content = content[0:re.search(banned_information, content).span()[0]-1].strip()\n",
    "        \n",
    "    # 清洗文章中冗余文字\n",
    "    for banned_word in banned_words:\n",
    "        content = content.replace(banned_word, '')\n",
    "    # print('文章内容获取完成！')\n",
    "    \n",
    "    # 将标题从洗词库中去除\n",
    "    banned_words.remove('【' + title + '】')\n",
    "\n",
    "    # 获取相关主题\n",
    "    related_stocks = []\n",
    "    for each in soup.find_all(class_='keytip'): \n",
    "        related_stocks.append(each.get_text())\n",
    "    \n",
    "    # 相关主题查重\n",
    "    related_stocks = list(set(related_stocks))\n",
    "    # print('相关主题单获取完成！')         \n",
    "    \n",
    "    # 写入SFrame\n",
    "    temp_sframe = tc.SFrame({'year':[str(time[0:4])],\n",
    "                             'month':[str(time[5:7])],\n",
    "                             'day':[str(time[8:10])],\n",
    "                             'date':[str(time[0:4]) + str(time[5:7]) + str(time[8:10])],\n",
    "                             'title':[title],\n",
    "                             'contents':[content], \n",
    "                             'related':[related_stocks]})\n",
    "    news = news.append(temp_sframe)\n",
    "    # print('SFrame写入完成!')\n",
    "    \n",
    "    # 释放内存\n",
    "    del(req, response, html, soup, time, content, related_stocks)\n",
    "    \n",
    "    # 刷新计数 \n",
    "    counter += 1\n",
    "    # print('计数刷新完成！')\n",
    "    # print('页面数据获取完毕！')\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取第1篇文章，共516篇.\n",
      "正在获取第2篇文章，共516篇.\n",
      "正在获取第3篇文章，共516篇.\n",
      "正在获取第4篇文章，共516篇.\n",
      "正在获取第5篇文章，共516篇.\n",
      "正在获取第6篇文章，共516篇.\n",
      "正在获取第7篇文章，共516篇.\n",
      "正在获取第8篇文章，共516篇.\n",
      "正在获取第9篇文章，共516篇.\n",
      "正在获取第10篇文章，共516篇.\n",
      "正在获取第11篇文章，共516篇.\n",
      "正在获取第12篇文章，共516篇.\n",
      "正在获取第13篇文章，共516篇.\n",
      "正在获取第14篇文章，共516篇.\n",
      "正在获取第15篇文章，共516篇.\n",
      "正在获取第16篇文章，共516篇.\n",
      "正在获取第17篇文章，共516篇.\n",
      "正在获取第18篇文章，共516篇.\n",
      "正在获取第19篇文章，共516篇.\n",
      "正在获取第20篇文章，共516篇.\n",
      "正在获取第21篇文章，共516篇.\n",
      "正在获取第22篇文章，共516篇.\n",
      "正在获取第23篇文章，共516篇.\n",
      "正在获取第24篇文章，共516篇.\n",
      "正在获取第25篇文章，共516篇.\n",
      "正在获取第26篇文章，共516篇.\n",
      "正在获取第27篇文章，共516篇.\n",
      "正在获取第28篇文章，共516篇.\n",
      "正在获取第29篇文章，共516篇.\n",
      "正在获取第30篇文章，共516篇.\n",
      "正在获取第31篇文章，共516篇.\n",
      "正在获取第32篇文章，共516篇.\n",
      "正在获取第33篇文章，共516篇.\n",
      "正在获取第34篇文章，共516篇.\n",
      "正在获取第35篇文章，共516篇.\n",
      "正在获取第36篇文章，共516篇.\n",
      "正在获取第37篇文章，共516篇.\n",
      "正在获取第38篇文章，共516篇.\n",
      "正在获取第39篇文章，共516篇.\n",
      "正在获取第40篇文章，共516篇.\n",
      "正在获取第41篇文章，共516篇.\n",
      "正在获取第42篇文章，共516篇.\n",
      "正在获取第43篇文章，共516篇.\n",
      "正在获取第44篇文章，共516篇.\n",
      "正在获取第45篇文章，共516篇.\n",
      "正在获取第46篇文章，共516篇.\n",
      "正在获取第47篇文章，共516篇.\n",
      "正在获取第48篇文章，共516篇.\n",
      "正在获取第49篇文章，共516篇.\n",
      "正在获取第50篇文章，共516篇.\n",
      "正在获取第51篇文章，共516篇.\n",
      "正在获取第52篇文章，共516篇.\n",
      "正在获取第53篇文章，共516篇.\n",
      "正在获取第54篇文章，共516篇.\n",
      "正在获取第55篇文章，共516篇.\n",
      "正在获取第56篇文章，共516篇.\n",
      "正在获取第57篇文章，共516篇.\n",
      "正在获取第58篇文章，共516篇.\n",
      "正在获取第59篇文章，共516篇.\n",
      "正在获取第60篇文章，共516篇.\n",
      "正在获取第61篇文章，共516篇.\n",
      "正在获取第62篇文章，共516篇.\n",
      "正在获取第63篇文章，共516篇.\n",
      "正在获取第64篇文章，共516篇.\n",
      "正在获取第65篇文章，共516篇.\n"
     ]
    }
   ],
   "source": [
    "# 初始化计数器\n",
    "counter = 1\n",
    "\n",
    "# 获取总任务数\n",
    "total = len(urllist)\n",
    "\n",
    "# 创建SFrame\n",
    "news = tc.SFrame({'year':['0000'],'month':['00'],'day':['00'],'date':['00000000'],'title':['Null Title'],'contents':['Null Contents'],'related':[['Null', 'Null']]})\n",
    "\n",
    "# 下载数据\n",
    "for each_article in urllist:\n",
    "    # print('=============================================')\n",
    "    print('正在获取第' + str(counter) + '篇文章，共' + str(total) + '篇.')\n",
    "    # print('---------------------------------------------')\n",
    "    news = collectNews(news, each_article, counter) \n",
    "    counter += 1\n",
    "    \n",
    "print('获取完毕！共获取了' + len(news['title']) + '篇文章!')\n",
    "\n",
    "# 删除占位符\n",
    "news = news[1:len(news['title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据\n",
    "filepath = '../DataSets/Eastmoney/News/China/'\n",
    "date = '20' + str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\"))\n",
    "news.save(filepath + 'CHINA' + date + '.csv', format='csv')\n",
    "print('成功保存数据文件！数据路径：' + filepath + 'CHINA' + '-' + date + '.csv')\n",
    "print('程序运行时间戳：20' \n",
    "      + str(datetime.datetime.now().strftime(\"%y\")) + '年'\n",
    "      + str(datetime.datetime.now().strftime(\"%m\")) + '月' \n",
    "      + str(datetime.datetime.now().strftime(\"%d\")) + '日' \n",
    "      + str(datetime.datetime.now().strftime(\"%H\")) + '时' \n",
    "      + str(datetime.datetime.now().strftime(\"%M\")) + '分' \n",
    "      + str(datetime.datetime.now().strftime(\"%S\")) + '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
