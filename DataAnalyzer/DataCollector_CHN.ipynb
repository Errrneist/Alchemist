{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 新闻数据采集器(国内板块)\n",
    "## 吴泓骏 [拂晓工作室](https://github.com/Errrneist/Alchemist)\n",
    "* 此程序将对指定股票数据进行收集并整理\n",
    "* 同时创建SFrame友好的CSV文件以及对数据进行清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "* [1] [Basics of SFrame](https://apple.github.io/turicreate/docs/api/generated/turicreate.SFrame.html#turicreate.SFrame)\n",
    "* [2] [Remove Multiple Substring from String](https://stackoverflow.com/questions/31273642/better-way-to-remove-multiple-words-from-a-string)\n",
    "* [3] [楼老师的Python分析红楼梦](https://zhuanlan.zhihu.com/p/29209681)\n",
    "* [4] [如何使用pyltp包进行中文分词](https://blog.csdn.net/sinat_26917383/article/details/77067515)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongjunwu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 导入库\n",
    "import urllib\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import turicreate as tc\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备阶段\n",
    "* 1. 定义用于搜寻的板块\n",
    "* 2. 创建一个包含25个链接的搜寻目录\n",
    "* 3. 创建一个包含20个Class的搜寻目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功创建包含 25 个页面链接的目录！\n",
      "成功创建包含 20 个Class的目录！\n"
     ]
    }
   ],
   "source": [
    "# 新闻板块(同时也会加入热门新闻的数据)\n",
    "china = 'cgnjj'  # 国内\n",
    "international = 'cgjjj'  # 国际\n",
    "\n",
    "# 定义基本常量\n",
    "url = 'http://finance.eastmoney.com/news/' + china + '.html' # 主目录板块\n",
    "\n",
    "# 创建一个二十五页的list\n",
    "page_list = []\n",
    "counter = 1\n",
    "\n",
    "while counter <= 25:\n",
    "    pageurl = 'http://finance.eastmoney.com/news/' + china\n",
    "    if counter != 1:\n",
    "        pageurl = pageurl + '_' + str(counter) + '.html'\n",
    "        page_list.append(pageurl)\n",
    "    else:\n",
    "        pageurl = pageurl + '.html'\n",
    "        page_list.append(pageurl)\n",
    "    counter += 1\n",
    "    \n",
    "print('成功创建包含 ' + str(len(page_list)) + ' 个页面链接的目录！')\n",
    "\n",
    "# 创建一个包含20个class的list\n",
    "counter = 0\n",
    "class_list = []\n",
    "\n",
    "while counter < 20:\n",
    "    class_list.append('newsTr' + str(counter))\n",
    "    counter += 1\n",
    "    \n",
    "print('成功创建包含 ' + str(len(class_list)) + ' 个Class的目录！')\n",
    "# print(class_list)  # Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得所有文章的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------获取所有文章超链接程序-----------------\n",
      "分析任务开始！\n",
      "第1页分析完成！现在一共获得了37篇文章的链接！\n",
      "第2页分析完成！现在一共获得了74篇文章的链接！\n",
      "第3页分析完成！现在一共获得了117篇文章的链接！\n",
      "第4页分析完成！现在一共获得了161篇文章的链接！\n",
      "第5页分析完成！现在一共获得了198篇文章的链接！\n",
      "第6页分析完成！现在一共获得了235篇文章的链接！\n",
      "第7页分析完成！现在一共获得了274篇文章的链接！\n",
      "第8页分析完成！现在一共获得了311篇文章的链接！\n",
      "第9页分析完成！现在一共获得了350篇文章的链接！\n",
      "第10页分析完成！现在一共获得了388篇文章的链接！\n",
      "第11页分析完成！现在一共获得了428篇文章的链接！\n",
      "第12页分析完成！现在一共获得了466篇文章的链接！\n",
      "第13页分析完成！现在一共获得了507篇文章的链接！\n",
      "第14页分析完成！现在一共获得了545篇文章的链接！\n",
      "第15页分析完成！现在一共获得了585篇文章的链接！\n",
      "第16页分析完成！现在一共获得了622篇文章的链接！\n",
      "第17页分析完成！现在一共获得了666篇文章的链接！\n",
      "第18页分析完成！现在一共获得了705篇文章的链接！\n",
      "第19页分析完成！现在一共获得了744篇文章的链接！\n",
      "第20页分析完成！现在一共获得了781篇文章的链接！\n",
      "第21页分析完成！现在一共获得了819篇文章的链接！\n",
      "第22页分析完成！现在一共获得了856篇文章的链接！\n",
      "第23页分析完成！现在一共获得了894篇文章的链接！\n",
      "第24页分析完成！现在一共获得了934篇文章的链接！\n",
      "第25页分析完成！现在一共获得了975篇文章的链接！\n",
      "全部分析完成！正在查重...\n",
      "任务完成！共获得了517篇文章的链接！\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 初始化urllist\n",
    "urllist = []\n",
    "year = '2018'  # Separate parameter\n",
    "counter = 1\n",
    "\n",
    "# 创造urllist\n",
    "print('-----------------获取所有文章超链接程序-----------------')\n",
    "print('分析任务开始！')\n",
    "for url in page_list:\n",
    "    req = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for each_url in soup.find_all('a', href=True):\n",
    "        if 'http://finance.eastmoney.com/news/' in each_url['href']:\n",
    "            if year in each_url['href']:\n",
    "                urllist.append(each_url['href'])   \n",
    "    print('第' + str(counter) + '页分析完成！现在一共获得了' + str(len(urllist)) + '篇文章的链接！')\n",
    "    counter += 1\n",
    "\n",
    "print('全部分析完成！正在查重...')\n",
    "urllist = list(set(urllist))\n",
    "print('任务完成！共获得了' + str(len(urllist)) + '篇文章的链接！')\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将各个网页的资料抓取进SFrame\n",
    "* 1. 新建用于抓取的函数\n",
    "* 2. 实施抓取\n",
    "* 3. 清洗数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建洗词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据的词库\n",
    "banned_info = ['责任编辑','原标题']\n",
    "banned_words = ['摘要\\n', '\\n', '\\r','\\u3000','（中国新闻网）',\n",
    "                '来源','以下简称', '（新华社）', '>>>', \n",
    "                '商务微新闻带你一图了解！','-', '经济日报', '中国经济网',\n",
    "                '附件：', '>>>>', '>>', '>>>>>', '（新华网）', \n",
    "                '（第一财经）', '据新华社报道，', '▼', '▲', '【','】',' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建抓取与清洗函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓取新闻URL函数\n",
    "def collectNews(news, url, counter):\n",
    "    # 准备请求数据\n",
    "    req = urllib.request.Request(url)\n",
    "    print('请求链接连接成功!')\n",
    "    response = urllib.request.urlopen(req)\n",
    "    print('收到反馈信号！')\n",
    "    html = response.read()\n",
    "    print('HTML生成完毕！')\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    print('SOUP创建完成！')\n",
    "    \n",
    "    # 获得文章的发表时间 \n",
    "    time = soup.find(class_=\"time\").get_text()\n",
    "    print('时间获取完毕！')\n",
    "    \n",
    "    # 获得文章的标题\n",
    "    title = soup.find('h1').get_text()\n",
    "    \n",
    "    # 鉴于有的文章将标题重复一遍 如此加入洗词库把标题洗掉\n",
    "    banned_words.append(title)\n",
    "    \n",
    "    # 获得文章的内容\n",
    "    content = soup.find(id=\"ContentBody\").get_text() \n",
    "    \n",
    "    # 清洗文章末尾编辑原文信息    \n",
    "    for banned_information in banned_info:\n",
    "        if banned_information in content:\n",
    "            content = content[0:re.search(banned_information, content).span()[0]-1].strip()\n",
    "        \n",
    "    # 清洗文章中冗余文字\n",
    "    for banned_word in banned_words:\n",
    "        content = content.replace(banned_word, '')\n",
    "    print('文章内容获取完成！')\n",
    "    \n",
    "    # 将标题从洗词库中去除\n",
    "    banned_words.remove(title)\n",
    "\n",
    "    # 获取相关主题\n",
    "    related_stocks = []\n",
    "    for each in soup.find_all(class_='keytip'): \n",
    "        related_stocks.append(each.get_text())\n",
    "    \n",
    "    # 相关主题查重\n",
    "    related_stocks = list(set(related_stocks))\n",
    "    print('相关主题单获取完成！')         \n",
    "    \n",
    "    # 写入SFrame\n",
    "    temp_sframe = tc.SFrame({'year':[str(time[0:4])],\n",
    "                             'month':[str(time[5:7])],\n",
    "                             'day':[str(time[8:10])],\n",
    "                             'date':[str(time[0:4]) + str(time[5:7]) + str(time[8:10])],\n",
    "                             'title':[title],\n",
    "                             'contents':[content], \n",
    "                             'related':[related_stocks]})\n",
    "    news = news.append(temp_sframe)\n",
    "    print('SFrame写入完成!')\n",
    "    \n",
    "    # 释放内存\n",
    "    del(req, response, html, soup, time, content, related_stocks)\n",
    "    \n",
    "    # 刷新计数 \n",
    "    counter += 1\n",
    "    print('计数刷新完成！')\n",
    "    print('页面数据获取完毕！')\n",
    "    return news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行抓取与数据清洗函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "正在获取第1篇文章，共517篇.\n",
      "---------------------------------------------\n",
      "请求链接连接成功!\n"
     ]
    }
   ],
   "source": [
    "# 初始化计数器\n",
    "counter = 1\n",
    "\n",
    "# 获取总任务数\n",
    "total = len(urllist)\n",
    "\n",
    "# 初始化SFrame\n",
    "news = tc.SFrame({'year':['0000'],'month':['00'],'day':['00'],'date':['00000000'],'title':['Null Title'],'contents':['Null Contents'],'related':[['Null', 'Null']]})\n",
    "\n",
    "# 下载数据\n",
    "for each_article in urllist:\n",
    "    print('=============================================')\n",
    "    print('正在获取第' + str(counter) + '篇文章，共' + str(total) + '篇.')\n",
    "    print('---------------------------------------------')\n",
    "    news = collectNews(news, each_article, counter) \n",
    "    counter += 1\n",
    "    \n",
    "print('获取完毕！共获取了' + str(len(news['title'])) + '篇文章!')\n",
    "\n",
    "# 删除占位符\n",
    "news = news[1:len(news['title'])]\n",
    "\n",
    "# 若仍然存在 则原地去世\n",
    "if(news[0]['year'] == '0000'):\n",
    "    print('未移除前:' + news[0]['title'])\n",
    "    news = news[1:len(news['title'])]\n",
    "    print('移除后:' + news[0]['title'])\n",
    "else:\n",
    "    print('占位符已确认移除！')\n",
    "\n",
    "# 最后一次查重\n",
    "news = news.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 盖上时间戳并保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功保存数据文件！数据路径：../DataSets/Eastmoney/News/China/CHINA-20180701-2148.csv\n",
      "程序运行时间戳：2018年07月01日21时48分41秒\n"
     ]
    }
   ],
   "source": [
    "# 保存数据\n",
    "filepath = '../DataSets/Eastmoney/News/China/'\n",
    "date = '20' + str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\"))\n",
    "news.save(filepath + 'CHINA' + date + '.csv', format='csv')\n",
    "print('成功保存数据文件！数据路径：' + filepath + 'CHINA' + '-' + date + '.csv')\n",
    "\n",
    "# 打印时间戳\n",
    "print('程序运行时间戳：20' \n",
    "      + str(datetime.datetime.now().strftime(\"%y\")) + '年'\n",
    "      + str(datetime.datetime.now().strftime(\"%m\")) + '月' \n",
    "      + str(datetime.datetime.now().strftime(\"%d\")) + '日' \n",
    "      + str(datetime.datetime.now().strftime(\"%H\")) + '时' \n",
    "      + str(datetime.datetime.now().strftime(\"%M\")) + '分' \n",
    "      + str(datetime.datetime.now().strftime(\"%S\")) + '秒')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
