{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言识别处理单元\n",
    "## [拂晓工作室](https://github.com/Errrneist/Alchemist)\n",
    "* 此程序可用于识别文字中的语言以及深度分析新闻概况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "* [1] [Apple - SFrame实例](https://apple.github.io/turicreate/docs/api/generated/turicreate.SFrame.html#turicreate.SFrame)\n",
    "* [2] [Apple - Turicreate中关于SFrame.apply()的文档](https://apple.github.io/turicreate/docs/api/generated/turicreate.SFrame.apply.html)\n",
    "* [0] [Apple - Turicreate使用TrimRareWords来快速定义高频词](https://apple.github.io/turicreate/docs/api/generated/turicreate.text_analytics.trim_rare_words.html?highlight=remove%20punctuation)\n",
    "* [0] [Apple - 用Turicreate生成Word Count Vector](https://apple.github.io/turicreate/docs/api/generated/turicreate.text_analytics.count_words.html)\n",
    "* [0] [Apple - 关于WordProcessing以及使用dict_trim_by_values(n)删除频率以下的词以及TF-IDF的描述](https://turi.com/learn/userguide/text/analysis.html)\n",
    "* [3] [Stackoverflow - Python从字符串中删除子字符串](https://stackoverflow.com/questions/31273642/better-way-to-remove-multiple-words-from-a-string)\n",
    "* [4] [Stackoverflow - 关于SFrame.apply()和Lambda x的实例](https://stackoverflow.com/questions/33028423/how-can-i-use-apply-with-a-function-that-takes-multiple-inputs)\n",
    "* [0] [Stackexchange - Merge two list and discarding duplicates](https://codereview.stackexchange.com/questions/108171/merge-two-list-and-discarding-duplicates)\n",
    "* [5] [楼宇 - 用Python分析《红楼梦》](https://zhuanlan.zhihu.com/p/29209681)\n",
    "* [0] [zhon - hanzi文档](http://zhon.readthedocs.io/en/latest/)\n",
    "* [6] [CSDN - 使用pyltp包进行中文分词实例](https://blog.csdn.net/sinat_26917383/article/details/77067515)\n",
    "* [7] [CSDN - NLP+语义分析: 角色标注、篇章分析](https://blog.csdn.net/sinat_26917383/article/details/55683599)\n",
    "* [8] [CSDN - NLP情感分析简介](https://blog.csdn.net/android_ruben/article/details/78174172)\n",
    "* [9] [CSDN - 基于机器学习的NLP情感分析（二）—- 分类问题](https://blog.csdn.net/stary_yan/article/details/75330729)\n",
    "* [0] [CSDN - 关于使用zhon.hanzi移除标点符号的解决方法](https://www.cnblogs.com/arkenstone/p/6092255.html)\n",
    "* [0] [CSDN - Python中正则表达式sub函数用法总结](https://blog.csdn.net/hzliyaya/article/details/52495150)\n",
    "* [0] [CSDN - Python中另一种移除标点符号的办法](https://blog.csdn.net/mach_learn/article/details/41744487)\n",
    "* [0] [CSDN - Python字典从大到小排序](https://blog.csdn.net/yangnianjinxin/article/details/79284318)\n",
    "* [0] [CSDN - 中文依存句法简介](https://blog.csdn.net/sinat_33741547/article/details/79258045)\n",
    "* [10] [PYLTP - pyltp技术文档](http://pyltp.readthedocs.io/zh_CN/latest/api.html#id15)\n",
    "* [11] [PYLTP - pyltp介绍文档一](https://www.ltp-cloud.com/intro/#introduction )\n",
    "* [12] [PYLTP - pyltp介绍文档二](http://ltp.readthedocs.io/zh_CN/latest/appendix.html#id5)\n",
    "* [13] [PYLTP - pyltp深度训练模型](https://pan.baidu.com/share/link?shareid=1988562907&uk=2738088569#list/path=/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A] 导入库模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入基本库\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# 网络获取相关包\n",
    "import pymysql\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 机器学习与大数据框架\n",
    "import turicreate as tc\n",
    "import csv\n",
    "from zhon.hanzi import punctuation\n",
    "\n",
    "# 自然语言识别框架\n",
    "import pyltp\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRoleLabeller\n",
    "LTP_DATA_DIR = '../../../LTP_data_v3.4.0/'  # ltp模型目录的路径\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [B] 载入数据模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于从上次停止工作后恢复数据\n",
    "news_path = '../DataSets/Eastmoney/News_NLP/China/'\n",
    "news_source = 'NLPCHINA20180702-1906'\n",
    "# news_source = 'NLPCHINA20180702-1906.csv'\n",
    "news = tc.SFrame(news_path + news_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.053299 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.053299 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,int,int,int,list,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/hongjunwu/Documents/GitHub/Alchemist/DataSets/Eastmoney/News/China/CHINA20180702-1906.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 509 lines in 0.042847 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 509 lines in 0.042847 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 载入纯净数据\n",
    "news_path = '../DataSets/Eastmoney/News/China/'\n",
    "news_source = 'CHINA20180702-1906.csv'\n",
    "news = tc.SFrame(news_path + news_source)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [C] 检查数据纯净度模块\n",
    "* 测试用，检查清洗数据的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 随机人工检查数据纯净度\n",
    "\n",
    "def showContents(number_to_show, total_amount):\n",
    "    counter = 0\n",
    "    while counter < number_to_show:\n",
    "        print(news['contents'][int(random.uniform(0, total_amount - 1))])\n",
    "        print('')\n",
    "        counter += 1\n",
    "    \n",
    "# 预览\n",
    "# showContents(3, len(news['contents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [D] 自然语言识别模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 分句模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分句函数\n",
    "* 将句子分开并创建名为'content_split_sents'的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分句函数\n",
    "def splitSents(contents):\n",
    "    return SentenceSplitter.split(contents)\n",
    "\n",
    "# 执行分句函数\n",
    "news['content_split_sents'] = news['contents'].apply(splitSents)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [2] 移除标点符号模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-4-708563a9aa48>:3: DeprecationWarning: invalid escape sequence \\s\n",
      "  return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", contents)\n",
      "<unknown>:2: DeprecationWarning: invalid escape sequence \\s\n"
     ]
    }
   ],
   "source": [
    "# 定义移除标点符号函数\n",
    "def removePunctuation(contents):\n",
    "    return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\", contents)\n",
    "\n",
    "# 运行移除函数并创建一个名为'contents_nopunc'的列\n",
    "news['contents_nopunc'] = news['contents'].apply(removePunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 分词函数模块\n",
    "* 将词分开 并创建名为'contents_split_words'的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词函数\n",
    "def splitWords(contents):\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "    words = segmentor.segment(contents)  # 分词\n",
    "    segmentor.release()  # 释放模型\n",
    "    return words\n",
    "\n",
    "# 执行分词函数\n",
    "news['content_split_words'] = news['contents_nopunc'].apply(splitWords)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] 词性标注模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagWords(contents_split):\n",
    "    postagger = Postagger() # 初始化实例\n",
    "    postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "    words = contents_split  # 分词结果\n",
    "    postags = postagger.postag(words)  # 词性标注\n",
    "\n",
    "    postagger.release()  # 释放模型\n",
    "    return postags\n",
    "\n",
    "# 执行标注函数\n",
    "news['content_tag_words'] = news['content_split_words'].apply(tagWords)\n",
    "\n",
    "# 预览\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] 命名实体识别模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讲究实体识别\n",
    "def recognizeWords(words):\n",
    "    # postags = news['content_split_words' == words]['content_tag_words']\n",
    "    recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "    recognizer.load(ner_model_path)  # 加载模型\n",
    "    netags = recognizer.recognize(words, news['content_split_words' == words]['content_tag_words'])  # 命名实体识别\n",
    "    recognizer.release()  # 释放模型\n",
    "    return netags\n",
    "\n",
    "# 执行实体识别\n",
    "news['recognized_words'] = news['content_split_words'].apply(recognizeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] 生成Word Count Vector模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成Word Count Vector\n",
    "news['word_count'] = tc.text_analytics.count_words(news['content_split_words'])\n",
    "\n",
    "# 整理Word Count Vector\n",
    "def sortWordCount(dictionary):\n",
    "    return sorted(dictionary.items(),key = lambda d:d[1],reverse=True)\n",
    "\n",
    "# 执行整理Word Count Vector\n",
    "news['sorted_word_count'] = news['word_count'].apply(sortWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7] 计算TF-IDF模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要生成TF-IDF模块\n",
    "\n",
    "# 生成空格函数\n",
    "def separateSpace(wordlist):\n",
    "    temp_str = ''\n",
    "    for word in wordlist:\n",
    "        temp_str = temp_str + word + ' '\n",
    "    return temp_str\n",
    "\n",
    "# 生成空格组成的str \n",
    "news['contents_space'] = news['content_split_words'].apply(separateSpace)\n",
    "\n",
    "# 通过由空格组成的str计算TF-IDF\n",
    "news['tf-idf'] = tc.text_analytics.tf_idf(news['contents_space'])\n",
    "\n",
    "# 整理TF-IDF\n",
    "def sortTFIDF(dictionary):\n",
    "    return sorted(dictionary.items(),key = lambda d:d[1],reverse=True)\n",
    "\n",
    "# 执行整理TF-IDF\n",
    "news['sorted_tf-idf'] = news['tf-idf'].apply(sortTFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF关键字排名模块\n",
    "def rankKeyTFIDF(tfidf_list):\n",
    "    origional_list = tfidf_list[0:10]\n",
    "    keyWord_list = []\n",
    "    for item in origional_list:\n",
    "        keyWord_list.append(item[0])\n",
    "    return keyWord_list\n",
    "\n",
    "news['ranked_tf-idf_keywords'] = news['sorted_tf-idf'].apply(rankKeyTFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] 依存句法分析模块（实验中）\n",
    "* 似乎还不支持保存为pyltp.ParsedWords格式……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存句法分析函数\n",
    "def parseWords(words):\n",
    "    postags = news['content_split_words' == words]['content_tag_words']\n",
    "    parser = Parser() # 初始化实例\n",
    "    parser.load(par_model_path)  # 加载模型\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "    parser.release()  # 释放模型\n",
    "    return arcs\n",
    "\n",
    "# 执行依存句法分析\n",
    "news['parsed_words'] = news['content_split_words'].apply(parseWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [9] 语义角色分析模块（实验中）\n",
    "* 似乎还不支持分析或保存为pyltp.ParsedWords格式……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语意角色分析函数\n",
    "def labelWords(words):\n",
    "    postags = news['content_split_words' == words]['content_tag_words']\n",
    "    netags = news['content_split_words' == words]['recognized_words']\n",
    "    arcs = news['content_split_words' == words]['parsed_words']\n",
    "    \n",
    "    labeller = SementicRoleLabeller() # 初始化实例\n",
    "    labeller.load(srl_model_path)  # 加载模型\n",
    "    roles = labeller.label(words, postags, netags, arcs)\n",
    "    labeller.release()  # 释放模型\n",
    "    return roles\n",
    "\n",
    "# 执行语意角色分析\n",
    "news['labeled_words'] = news['content_split_words'].apply(labelWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] 创建关键字模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two list and discarding duplicates [Stackexchange]\n",
    "def merge_no_duplicates(iterable_1, iterable_2):\n",
    "    myset = set(iterable_1).union(set(iterable_2))\n",
    "    return sorted(list(myset))\n",
    "\n",
    "# 将TFIDF筛选出的和从网站本身下载的超链关键字放进一个关键字SArray\n",
    "def makeKeyWords(related):\n",
    "    ranked_tfidf = news['related' == related]['ranked_tf-idf_keywords']\n",
    "    return merge_no_duplicates(related, ranked_tfidf)\n",
    "\n",
    "# 执行创建关键字\n",
    "news['keywords'] = news['related'].apply(makeKeyWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] 预览SFrame模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] 随机检查成果模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前显示第45条新闻：\n",
      "----------------------------------------------------------------\n",
      "据工信部网站消息，日前工信部、应急管理部、财政部、科技部联合发布了《关于加快安全产业发展的指导意见》。意见指出，安全产业是为安全生产、防灾减灾、应急救援等安全保障活动提供专用技术、产品和服务的产业，是国家重点支持的战略产业。发展安全产业对于落实安全发展理念、提升全社会安全保障能力和本质安全水平、推动经济高质量发展、培育新经济增长点具有重要意义。为落实《中共中央国务院关于推进安全生产领域改革发展的意见》(中发〔2016〕32号)，现就安全产业发展提出如下意见。一、总体要求(一)指导思想全面贯彻党的十九大精神，以习近平新时代中国特色社会主义思想为指导，牢固树立安全发展理念，弘扬生命至上、安全第一的思想，聚焦风险隐患源头治理，以坚决遏制重特大安全生产事故为目标，以提升安全保障能力为重点，以示范工程为依托，着力推广先进安全技术、产品和服务，提升各行业领域的本质安全水平；以企业为主体，市场为导向，强化政府引导，着力推动安全产业创新发展、集聚发展，积极培育新的经济增长点。(二)基本原则创新驱动，优化供给。加快关键、亟需新技术新产品研发，提高安全产品供给质量，不断缩小与国际先进水平差距；加快推动商业模式创新，深化产融合作，积极培育安全服务新业态。突出重点，集聚发展。聚焦安全生产事故高发、频发的重点行业领域，优先发展可有效防范事故、具有重大推广应用价值的专用技术与产品；提高产业集中度，完善产业链，促进产业发展规模化、专业化、集聚集约化。需求牵引，示范带动。提升安全标准，强化安全监管，激发市场需求，推广先进可靠的安全产品和服务；面向重点行业领域，坚持问题导向，实施安全产品试点示范应用工程，引导社会资本投入，有力拉动安全产业发展。规范引导，有序推进。充分发挥市场在资源配置中的决定性作用，调动市场主体发展安全产业的积极性；加强行业自律，规范市场秩序，营造有利于安全产业健康发展的市场环境。(三)工作目标到2020年，安全产业体系基本建立，产业销售收入超过万亿元。先进安全产品有效供给能力显著提高，在重点行业领域实现示范应用。创新能力明显提高。突破一批保障生产安全、城市公共安全的关键核心技术，研发一批具有国际先进水平的安全与应急产品，推广应用一批“机械化换人、自动化减人”的安全技术装备。集聚效应初步显现。创建10家以上国家安全产业示范园区，培育2家以上具有较强国际竞争力的骨干企业和知名品牌，打造百家专业化的创新型中小企业。发展环境持续优化。技术创新、标准、投融资服务、产业链协作以及政策保障等产业支撑体系初步建立，一个有利于产业健康发展的市场环境基本形成。行业应用不断深化。组织实施一批试点示范工程，在交通运输、矿山、危险化学品、工程施工、重大基础设施、城市公共安全等重点行业领域推广应用一批具有基础性、紧迫性的安全产品，为遏制重特大事故提供有力保障。到2025年，安全产业成为国民经济新的增长点，部分领域产品技术达到国际领先水平；国家安全产业示范园区和国际知名品牌建设成果显著，初步形成若干世界级先进安全装备制造集群；安全与应急技术装备在重点行业领域得到规模化应用，社会本质安全水平显著提高。二、发展方向面向生产安全和城市公共安全的保障需求，制定目录、清单，优化产品结构，引导产业发展，创新服务业态。(一)加快先进安全产品研发和产业化风险监测预警产品。生产安全领域，重点发展交通运输、矿山开采、工程施工、危险品生产储存、重大基础设施等方面的监测预警产品和故障诊断系统。城市安全领域，重点发展高危场所、高层建筑、超大综合体、城市管网、地下空间、人员密集场所等方面的监测预警产品。安全防护防控产品。生产安全领域，重点发展用于高危作业场所的工业机器人(换人)、人机隔离智能化控制系统(减人)、尘毒危害自动处理与自动隔抑爆等安全防护装置或部件、交通运输领域的主被动安全产品和安全防护设施等。城市安全领域，重点发展智能化巡检、集成式建筑施工平台、智能安防系统等安全防控产品。综合安全防护领域，重点发展电气安全产品、高效环保的阻燃防爆材料及各类防护产品等。应急处置救援产品。应急处置方面，重点发展应急指挥、通信、供电和逃生避险等产品，以及危险品泄漏等应急处置装备。应急救援方面，重点发展各类搜救、破拆、消防等智能化救援装备。(二)积极培育安全服务新业态在规范发展安全工程设计与监理、标准规范制订、检测与认证、评估与评价、事故分析与鉴定等传统安全服务基础上，积极发展安全管理与技术咨询、产品展览展示、教育培训与体验、应急演练演示等与国外存在较大差距的安全服务，重点发展基于物联网、大数据、人工智能等技术的智慧安全云服务。三、重点任务组织实施“5+N”计划，逐步健全技术创新、标准、投融资服务、产业链协作和政策五大支撑体系，开展N项示范工程建设，培育市场需求，壮大产业规模。(一)健全产业技术创新支撑体系建设一批高水平科技创新基地。按照国家科技创新基地总体部署，推动国家重点实验室建设和优化整合，大幅提升安全产业领域持续创新能力。组建若干个细分领域安全技术创新联盟，推动安全技术示范应用、科学普及与教育培训基地建设，逐步形成国家安全科技示范网络和成果推广体系。攻克一批产业前沿和共性技术。聚焦重点行业领域安全需求，以数字化、网络化、智能化安全技术与装备科研为重点方向，通过中央财政科技计划(专项、基金等)支持符合条件的灾害防治、预测预警、监测监控、个体防护、应急救援、本质安全工艺和装备、安全服务等关键技术的研发。加强安全技术成果转移转化。通过创投基金等渠道支持转化一批先进适用安全技术和产品。鼓励地方政府完善科技成果转化激励制度，健全科技成果评估和市场定价机制，提升科技创新和成果转化效率。(二)健全产业相关标准体系建立完善产业相关标准体系。全面梳理安全技术装备标准建设的需求和存在的问题，完善包括强制性国家标准、推荐性国家标准、行业和地方标准、团体标准、企业标准等在内的标准体系框架，建立政府主导制定与市场自主制定的标准协同发展、协调配套的新型标准体系，促进产品和服务推广应用。制修订一批关键亟需的技术和产品标准。按照“急用先行、逐步完善”的原则，面向重点行业领域，推动一批安全技术、产品的强制性标准制修订，组织制修订相关安全产品行业标准，鼓励制定相关团体标准，并组织标准的宣贯和培训。制修订重点领域安全生产标准。根据安全生产执法检查发现的突出问题、事故原因分析和新工艺技术装备应用等情况，及时制修订安全生产标准，提高重点行业领域安全生产标准，推动先进安全装备应用。(三)健全投融资服务体系探索建立政策引导、市场化运作的投资服务体系。鼓励地方将安全产业纳入政府基金投资范畴，引导金融机构等积极参与地方安全产业发展投资基金和行业安全产业发展投资基金；引导股权投资基金、创业投资基金等各类民间资本为企业发展、安全产业园区建设和智慧社会安全基础保障能力建设等提供支持。推动企业利用多层次资本市场进行融资。鼓励企业按照国家相关政策在资本市场进行股权融资，以发行公司债券、资产支持证券等方式进行债权融资。鼓励金融研究机构开展安全产业指数研究，引导社会资本关注安全产业。积极发展安全装备融资租赁服务。引导国内大型融资租赁机构与安全装备生产企业组建融资租赁服务联合体，通过融资租赁等方式，为企业生产安全、城市公共安全等提供大型安全装备、基础设施等融资租赁服务。(四)完善产业链协作体系建设安全产业大数据平台。依托制造强国产业基础大数据平台，构建多方合作、共建共享的国家安全产业基础数据库。基于云计算和大数据分析技术，面向各类市场主体提供供应链合作、经济运行分析、技术和市场发展趋势研判、产业区域布局优化、示范应用、政策效果评估等公共服务。继续开展国家安全产业示范园区创建。编制发布《国家安全产业示范园区创建指南》，鼓励有条件的地区发展各具特色的安全产业集聚区，形成区域性安全产业链。在示范园区基础上，择优建设一批安全产业国家新型工业化产业示范基地，逐步培育成为具有国际影响力的先进安全装备制造集群。建设安全产业公共服务平台。依托现有社会公共服务资源，选择一批基础好、信誉高的技术服务机构，扶持建设一批公共服务平台，规范服务标准，提升服务质量，增强对园区建设、产业链协同发展等方面的支撑作用。大力发展服务型制造。支持地方政府、园区、企业积极发展本质安全工艺和产品设计服务、安全装备(系统)定制化服务、全生命周期安全管理服务等服务型制造，对接科技、金融等多种资源，创新商业模式，引导企业深度参与上下游产业链协同和社会协作。(五)完善政策体系完善产业支持政策。充分利用现有资金渠道，引导和鼓励社会资本加大对安全产业相关领域的支持力度。发挥国家安全产业基础数据库作用，每年遴选一批先进安全产品编入《推广先进与淘汰落后安全技术装备目录》，增强对企业安全设施改造升级和风险隐患治理、示范工程建设、社会资本投资等方面的指导作用。落实企业安全生产费用提取与使用管理制度，鼓励企业应用先进适用的安全技术、产品和服务，提升安全基础保障能力。探索安全产业与保险业合作机制。利用首台(套)保险补偿机制支持符合条件的重点行业领域重大安全技术装备。鼓励地方政府和企业在国家保险政策支持范围内与保险企业开展合作，吸引保险资金参与重点行业领域和区域性安全产品示范工程建设及安全基础设施建设。鼓励安全产品研发制造企业与保险企业开展合作，创新商业模式、销售渠道和产品服务等，加速推动先进安全技术、产品和服务的规模化应用。(六)建设N项试点示范工程编制安全产品推广应用三年行动计划。根据我国安全生产形势变化，从国家安全产业基础数据库中筛选出一批安全产品，制定安全产品推广应用行动计划，确定行动目标、实施方案和进度安排等事项。组织开展先进安全产品应用示范。面向交通运输、矿山开采、工程施工、危险品、重大基础设施和城市安全等重点行业领域，会同国务院相关部门和地方政府组织建设N项国家、省、市级先进安全产品应用示范工程，逐步探索有效的经验和模式，不断完善后在相关领域推广。四、营造有利发展环境(一)加强组织领导工业和信息化部、应急管理部、财政部、科技部将建立沟通协调机制，加强组织领导，加强与国务院有关部门在政策、规划、法规、标准、市场准入等各方面的协调沟通。各地相关部门要参照本意见要求，制定促进本省(区、市)安全产业发展的政策措施，充分发挥行业协会、产业联盟等中介机构的桥梁纽带作用，促进安全产业有序、健康、可持续发展。(二)加强国际合作鼓励企业加强国际科技创新合作，引进、消化、吸收、再创新国外先进安全技术和服务理念；鼓励企业、技术服务机构积极参与国际标准制定，牵头或参与建立国际安全产业创新联盟。鼓励企业参与并购、合资、参股国际先进安全科技企业或设立海外研发中心；鼓励安全装备企业和安全服务企业以服务“一带一路”建设和国际产能合作为重点，积极开拓国际市场。鼓励国外创新资源与国内安全产业创新发展需求开展对接，促进国际先进安全科技成果转移转化。(三)加强人才培养合理利用高等院校和科技资源，吸纳高素质人员进入安全科技领域，加强安全科学与工程学科建设和高层次专业人才队伍培养。依托重点企业、行业协会开展安全领域急需紧缺人才培养，鼓励社会培训机构开展面向安全产业专业人才培训。支持相关高校开展安全产业相关学科专业建设，推动校企协同，改进产教融合、校企合作办学模式，加强安全领域复合型人才培养。(四)加强宣传教育组织召开中国安全产业大会和安全装备博览会，通过会展集聚带动产业集聚，推进产研对接、产需对接、产融对接。鼓励相关部门和机构建设宣传培训演练基地，编写出版安全教材与科普手册，摄制安全生产公益广告和警示教育片；充分利用广播、电视、网络、报纸、卫星传输平台、新媒体等平台，加强安全知识宣传，提高全民安全意识、知识水平和避险自救能力。\n",
      "----------------------------------------------------------------\n",
      "['安全', '产业', '产品', '示范', '先进', '应急', '应用', '重点', '技术', '装备']\n",
      "----------------------------------------------------------------\n",
      "['保险', '新基', '新和成', '工程建设', '机器人']\n"
     ]
    }
   ],
   "source": [
    "def printRank(num):\n",
    "    print('目前显示第' + str(num) + '条新闻：')\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(news[num]['contents'])\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(news[num]['ranked_tf-idf_keywords'])\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(news[num]['related'])\n",
    "    \n",
    "printRank(random.randint(0,len(news['title'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] 保存数据模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序运行时间戳：2018年07月04日20时46分27秒\n",
      "\n",
      "成功保存自然语言识别数据文件！数据路径：\n",
      "../DataSets/Eastmoney/News_NLP/China/NLPCHINA20180702-1906\n"
     ]
    }
   ],
   "source": [
    "# 保存数据\n",
    "filepath = '../DataSets/Eastmoney/News_NLP/China/'\n",
    "news_source = 'NLPCHINA20180702-1906'\n",
    "news.save(filepath + news_source)\n",
    "# news.save(filepath + 'NLP' + news_source, format='csv')\n",
    "\n",
    "# 打印时间戳\n",
    "print('程序运行时间戳：20' \n",
    "      + str(datetime.datetime.now().strftime(\"%y\")) + '年'\n",
    "      + str(datetime.datetime.now().strftime(\"%m\")) + '月' \n",
    "      + str(datetime.datetime.now().strftime(\"%d\")) + '日' \n",
    "      + str(datetime.datetime.now().strftime(\"%H\")) + '时' \n",
    "      + str(datetime.datetime.now().strftime(\"%M\")) + '分' \n",
    "      + str(datetime.datetime.now().strftime(\"%S\")) + '秒')\n",
    "\n",
    "# 打印数据路径\n",
    "print('\\n成功保存自然语言识别数据文件！数据路径：')\n",
    "print(filepath + news_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
